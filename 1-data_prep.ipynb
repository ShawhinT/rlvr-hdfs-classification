{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee74f20d-e0e0-4c69-99d7-d7cd3df2b719",
   "metadata": {},
   "source": [
    "# Data Preperation\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada5f8a-e6be-4a83-8ae2-9d89125ccc33",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34185f3-9ab9-4104-bbca-618370f9f8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb68dab-64af-4436-9b80-b7ad16fff0e6",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abdb08d-3eab-428e-a0f7-ffd16faf459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from HF hub\n",
    "ds = load_dataset(\"logfit-project/HDFS_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c845ff0c-0e7e-4605-ad09-214e6651e7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['line_number', 'date', 'time', 'pid', 'level', 'component', 'content', 'block_id', 'anomaly'],\n",
       "        num_rows: 11175629\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404aad4-adb7-4f8b-a6f9-bdefb5ffc0e0",
   "metadata": {},
   "source": [
    "### transform to block level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26572dd-e019-44b1-853b-344aafe6995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (11175629, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_number</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>pid</th>\n",
       "      <th>level</th>\n",
       "      <th>component</th>\n",
       "      <th>content</th>\n",
       "      <th>block_id</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>081109</td>\n",
       "      <td>203518</td>\n",
       "      <td>35</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.FSNamesystem</td>\n",
       "      <td>BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>143</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$DataXceiver</td>\n",
       "      <td>Receiving block blk_-1608999687919862906 src: ...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>081109</td>\n",
       "      <td>203519</td>\n",
       "      <td>145</td>\n",
       "      <td>INFO</td>\n",
       "      <td>dfs.DataNode$PacketResponder</td>\n",
       "      <td>PacketResponder 1 for block blk_-1608999687919...</td>\n",
       "      <td>blk_-1608999687919862906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   line_number    date    time  pid level                     component  \\\n",
       "0            1  081109  203518  143  INFO      dfs.DataNode$DataXceiver   \n",
       "1            2  081109  203518   35  INFO              dfs.FSNamesystem   \n",
       "2            3  081109  203519  143  INFO      dfs.DataNode$DataXceiver   \n",
       "3            4  081109  203519  145  INFO      dfs.DataNode$DataXceiver   \n",
       "4            5  081109  203519  145  INFO  dfs.DataNode$PacketResponder   \n",
       "\n",
       "                                             content  \\\n",
       "0  Receiving block blk_-1608999687919862906 src: ...   \n",
       "1  BLOCK* NameSystem.allocateBlock: /mnt/hadoop/m...   \n",
       "2  Receiving block blk_-1608999687919862906 src: ...   \n",
       "3  Receiving block blk_-1608999687919862906 src: ...   \n",
       "4  PacketResponder 1 for block blk_-1608999687919...   \n",
       "\n",
       "                   block_id  anomaly  \n",
       "0  blk_-1608999687919862906        0  \n",
       "1  blk_-1608999687919862906        0  \n",
       "2  blk_-1608999687919862906        0  \n",
       "3  blk_-1608999687919862906        0  \n",
       "4  blk_-1608999687919862906        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pandas df\n",
    "df = ds['train'].to_pandas()\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "o4ps721k9j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block-level dataset shape: (575061, 3)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    558223\n",
      "1     16838\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>block_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blk_-1000002529962039464</td>\n",
       "      <td>INFO dfs.DataNode$DataXceiver: Receiving block...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blk_-100000266894974466</td>\n",
       "      <td>INFO dfs.FSNamesystem: BLOCK* NameSystem.alloc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blk_-1000007292892887521</td>\n",
       "      <td>INFO dfs.DataNode$DataXceiver: Receiving block...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blk_-1000014584150379967</td>\n",
       "      <td>INFO dfs.DataNode$DataXceiver: Receiving block...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blk_-1000028658773048709</td>\n",
       "      <td>INFO dfs.DataNode$DataXceiver: Receiving block...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   block_id  \\\n",
       "0  blk_-1000002529962039464   \n",
       "1   blk_-100000266894974466   \n",
       "2  blk_-1000007292892887521   \n",
       "3  blk_-1000014584150379967   \n",
       "4  blk_-1000028658773048709   \n",
       "\n",
       "                                                text  label  \n",
       "0  INFO dfs.DataNode$DataXceiver: Receiving block...      0  \n",
       "1  INFO dfs.FSNamesystem: BLOCK* NameSystem.alloc...      0  \n",
       "2  INFO dfs.DataNode$DataXceiver: Receiving block...      0  \n",
       "3  INFO dfs.DataNode$DataXceiver: Receiving block...      0  \n",
       "4  INFO dfs.DataNode$DataXceiver: Receiving block...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format each line as \"<LEVEL> <COMPONENT>: <CONTENT>\"\n",
    "df['formatted'] = df['level'] + ' ' + df['component'] + ': ' + df['content']\n",
    "\n",
    "# sort by line_number to ensure correct order, then aggregate by block_id\n",
    "df = df.sort_values(['block_id', 'line_number'])\n",
    "\n",
    "df_blocks = df.groupby('block_id').agg(\n",
    "    text=('formatted', lambda x: '\\n'.join(x)),\n",
    "    label=('anomaly', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Block-level dataset shape: {df_blocks.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df_blocks['label'].value_counts())\n",
    "df_blocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6be05c-930b-4e31-a034-73ab9f38fff7",
   "metadata": {},
   "source": [
    "### see example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "v5ie0iwnaln",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block ID: blk_-1000002529962039464\n",
      "Label: 0\n",
      "\n",
      "Concatenated text (13 lines):\n",
      "INFO dfs.DataNode$DataXceiver: Receiving block blk_-1000002529962039464 src: /10.251.123.1:41333 dest: /10.251.123.1:50010\n",
      "INFO dfs.DataNode$DataXceiver: Receiving block blk_-1000002529962039464 src: /10.251.123.1:53174 dest: /10.251.123.1:50010\n",
      "INFO dfs.DataNode$DataXceiver: Receiving block blk_-1000002529962039464 src: /10.251.202.181:32980 dest: /10.251.202.181:50010\n",
      "INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_200811101024_0015_m_001261_0/part-01261. blk_-1000002529962039464\n",
      "INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1000002529962039464 terminating\n",
      "INFO dfs.DataNode$PacketResponder: Received block blk_-1000002529962039464 of size 3553241 from /10.251.123.1\n",
      "INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1000002529962039464 terminating\n",
      "INFO dfs.DataNode$PacketResponder: Received block blk_-1000002529962039464 of size 3553241 from /10.251.202.181\n",
      "INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.126.22:50010 is added to blk_-1000002529962039464 size 3553241\n",
      "INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.202.181:50010 is added to blk_-1000002529962039464 size 3553241\n",
      "INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.123.1:50010 is added to blk_-1000002529962039464 size 3553241\n",
      "INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1000002529962039464 terminating\n",
      "INFO dfs.DataNode$PacketResponder: Received block blk_-1000002529962039464 of size 3553241 from /10.251.123.1\n"
     ]
    }
   ],
   "source": [
    "sample_block = df_blocks.iloc[0]\n",
    "\n",
    "print(f\"Block ID: {sample_block['block_id']}\")\n",
    "print(f\"Label: {sample_block['label']}\")\n",
    "print(f\"\\nConcatenated text ({len(sample_block['text'].split(chr(10)))} lines):\")\n",
    "print(sample_block['text'][:2000] + \"...\" if len(sample_block['text']) > 2000 else sample_block['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a03c9-7290-4f13-a8f9-32670e56d4ca",
   "metadata": {},
   "source": [
    "### create train-dev-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0mc6nrwusx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 460048 (80.0%)\n",
      "Dev:   57506 (10.0%)\n",
      "Test:  57507 (10.0%)\n",
      "\n",
      "Label distribution:\n",
      "Train - Normal: 446578, Anomaly: 13470\n",
      "Dev   - Normal: 55822, Anomaly: 1684\n",
      "Test  - Normal: 55823, Anomaly: 1684\n"
     ]
    }
   ],
   "source": [
    "# Stratified 80/10/10 split using pandas\n",
    "def stratified_split(df, train_frac=0.8, dev_frac=0.1, seed=42):\n",
    "    train_parts, dev_parts, test_parts = [], [], []\n",
    "    \n",
    "    for label in df['label'].unique():\n",
    "        group = df[df['label'] == label].sample(frac=1, random_state=seed)\n",
    "        n = len(group)\n",
    "        train_end = int(n * train_frac)\n",
    "        dev_end = int(n * (train_frac + dev_frac))\n",
    "        \n",
    "        train_parts.append(group.iloc[:train_end])\n",
    "        dev_parts.append(group.iloc[train_end:dev_end])\n",
    "        test_parts.append(group.iloc[dev_end:])\n",
    "    \n",
    "    return pd.concat(train_parts), pd.concat(dev_parts), pd.concat(test_parts)\n",
    "\n",
    "train_df, dev_df, test_df = stratified_split(df_blocks)\n",
    "\n",
    "print(f\"Train: {len(train_df)} ({len(train_df)/len(df_blocks)*100:.1f}%)\")\n",
    "print(f\"Dev:   {len(dev_df)} ({len(dev_df)/len(df_blocks)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(test_df)} ({len(test_df)/len(df_blocks)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(f\"Train - Normal: {(train_df['label']==0).sum()}, Anomaly: {(train_df['label']==1).sum()}\")\n",
    "print(f\"Dev   - Normal: {(dev_df['label']==0).sum()}, Anomaly: {(dev_df['label']==1).sum()}\")\n",
    "print(f\"Test  - Normal: {(test_df['label']==0).sum()}, Anomaly: {(test_df['label']==1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17dc1e4-c6cb-420e-bdad-fbade3518856",
   "metadata": {},
   "source": [
    "### push to HF hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644f98cf-def2-4a8a-bcb0-a0d5f4392c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['block_id', 'text', 'label'],\n",
       "        num_rows: 460048\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['block_id', 'text', 'label'],\n",
       "        num_rows: 57506\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['block_id', 'text', 'label'],\n",
       "        num_rows: 57507\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pandas DataFrames to HuggingFace Datasets\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df, preserve_index=False),\n",
    "    'dev': Dataset.from_pandas(dev_df, preserve_index=False),\n",
    "    'test': Dataset.from_pandas(test_df, preserve_index=False)\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e9404ejnnn",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f9a0f91dd9458db72a2ea3304bfb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffa87b913234749b6c8b682f22eeeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9977b3c4174149feb9a514781e9a9d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ef40ed4854463a9b0d1fece7eef47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba9e211c2444dc9b8f0aabba887888f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15616ec8ffc4b5d8869cb5a9272b113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da659d6713843f1a771346eeac63798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5291ccfee764218b60a3a1f363107d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c065e4c6f344d66a31e6002aa6ab9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae1c21c76f2470c9ee3401dd0cfa888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8135dbd46a4a9aaed5185df0a1a09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b5740b8f5a4f9b904cc5d71ea50277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f5614af28940ad87f3e16d9f9b833b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39208e6d23994b92bdebf604dbf4c143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec6091e06dc4cb6a18ba442289c3151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83534aa26ad74943878726ca6463b70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560914b329724da7a9f931db563e7cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056b05227d77436f917d86803293e75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/shawhin/HDFS_v1_blocks/commit/fb0d0c9ee4dffe4455d2d0b0abe311e7739cd18f', commit_message='Upload dataset', commit_description='', oid='fb0d0c9ee4dffe4455d2d0b0abe311e7739cd18f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/shawhin/HDFS_v1_blocks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='shawhin/HDFS_v1_blocks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to HuggingFace Hub (requires: huggingface-cli login)\n",
    "dataset_dict.push_to_hub(\"shawhin/HDFS_v1_blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
