{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506a3951-5091-4354-9569-0697bb431de4",
   "metadata": {},
   "source": [
    "# Compare Base vs Fine-Tuned Model\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5d633",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f5e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from functions import run_inference, calculate_metrics, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61048f29",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921ab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from HF hub\n",
    "ds = load_dataset(\"shawhin/HDFS_v1_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf27a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 2 anomalous, 48 normal\n"
     ]
    }
   ],
   "source": [
    "num_test = 50\n",
    "test_sample = ds[\"test\"].shuffle(seed=0).select(range(num_test))\n",
    "\n",
    "# check class distribution in samples\n",
    "test_positive = sum(test_sample[\"label\"])\n",
    "\n",
    "print(f\"Test: {test_positive} anomalous, {len(test_sample) - test_positive} normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59de0a4",
   "metadata": {},
   "source": [
    "### response format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab3eaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define structured output schema\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"anomaly_flag\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"Anomalous\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"True if the log block is anomalous, false otherwise.\"\n",
    "                },\n",
    "                \"reasoning\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Explanation of the decision regarding whether the block is anomalous.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"Anomalous\", \"reasoning\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42473f",
   "metadata": {},
   "source": [
    "### evaluate base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "107a6af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running o4-mini-2025-04-16: 100%|██████████| 50/50 [04:29<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model (o4-mini-2025-04-16) metrics:\n",
      "  accuracy: 0.6600\n",
      "  precision: 0.0000\n",
      "  recall: 0.0000\n",
      "  f1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = \"o4-mini-2025-04-16\"\n",
    "\n",
    "# run base model on validation set\n",
    "test_labels = [bool(ex[\"label\"]) for ex in test_sample]\n",
    "\n",
    "base_predictions = run_inference(base_model, test_sample, response_format)\n",
    "base_metrics = calculate_metrics(base_predictions, test_labels)\n",
    "\n",
    "print(f\"Base model ({base_model}) metrics:\")\n",
    "for metric, value in base_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c905a7",
   "metadata": {},
   "source": [
    "### evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eeacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = \"\"\n",
    "\n",
    "# run fine-tuned model on validation set\n",
    "ft_predictions = run_inference(ft_model, test_sample, response_format)\n",
    "ft_metrics = calculate_metrics(ft_predictions, test_labels)\n",
    "\n",
    "print(f\"Fine-tuned model ({ft_model}) metrics:\")\n",
    "for metric, value in ft_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2142b4a",
   "metadata": {},
   "source": [
    "### compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare base vs fine-tuned model\n",
    "comparison = pd.DataFrame({\n",
    "    \"Base Model\": base_metrics,\n",
    "    \"Fine-tuned Model\": ft_metrics\n",
    "}).T\n",
    "\n",
    "# add improvement row\n",
    "improvement = {k: ft_metrics[k] - base_metrics[k] for k in base_metrics}\n",
    "comparison.loc[\"Improvement\"] = improvement\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison.round(4).to_string())\n",
    "\n",
    "print(\"\\nConfusion Matrices:\")\n",
    "print(f\"\\nBase Model:\\n{confusion_matrix(test_labels, base_predictions)}\")\n",
    "print(f\"\\nFine-tuned Model:\\n{confusion_matrix(test_labels, ft_predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
