{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Local RLVR Training with TRL GRPOTrainer\n",
        "\n",
        "This notebook fine-tunes LiquidAI's LFM2.5-1.2B-Thinking model for HDFS log anomaly detection using **Group Relative Policy Optimization (GRPO)** via the TRL library.\n",
        "\n",
        "Code authored by: Shaw Talebi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, PeftModel\n",
        "import torch\n",
        "from functions import run_local_inference, calculate_metrics, parse_prediction\n",
        "\n",
        "# Check device\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS (Apple Silicon)\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n",
        "\n",
        "Load from `shawhin/HDFS_v1_blocks` and sample for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"shawhin/HDFS_v1_blocks\")\n",
        "print(f\"Train: {len(dataset['train'])} examples\")\n",
        "print(f\"Validation: {len(dataset['dev'])} examples\")\n",
        "print(f\"Test: {len(dataset['test'])} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample train and validation datasets\n",
        "SAMPLE_SIZE = 100\n",
        "\n",
        "train_sample = dataset[\"train\"].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
        "val_sample = dataset[\"dev\"].shuffle(seed=42).select(range(SAMPLE_SIZE))\n",
        "\n",
        "print(f\"Train sample: {len(train_sample)} examples\")\n",
        "print(f\"Val sample: {len(val_sample)} examples\")\n",
        "\n",
        "# Check class distribution\n",
        "train_anomalous = sum(train_sample[\"label\"])\n",
        "val_anomalous = sum(val_sample[\"label\"])\n",
        "print(f\"\\nTrain anomalous: {train_anomalous}/{len(train_sample)} ({100*train_anomalous/len(train_sample):.1f}%)\")\n",
        "print(f\"Val anomalous: {val_anomalous}/{len(val_sample)} ({100*val_anomalous/len(val_sample):.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Format Dataset for GRPOTrainer\n",
        "\n",
        "Transform to TRL conversational format with `prompt` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are an HDFS log anomaly detector. Analyze the log block and determine if it indicates an anomaly.\\\n",
        "\\\n",
        "Think through your analysis step by step, then provide your final answer.\n",
        "End your response with exactly 'Anomalous: True' or 'Anomalous: False'.\"\"\"\n",
        "\n",
        "def format_for_grpo(example):\n",
        "    \"\"\"Format example for GRPOTrainer with conversational prompt.\"\"\"\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": f\"{example['text']}\"}\n",
        "        ],\n",
        "        \"label\": example[\"label\"]  # Keep for reward function\n",
        "    }\n",
        "\n",
        "train_dataset = train_sample.map(format_for_grpo, remove_columns=[\"block_id\", \"text\"])\n",
        "val_dataset = val_sample.map(format_for_grpo, remove_columns=[\"block_id\", \"text\"])\n",
        "\n",
        "print(\"Sample formatted prompt:\")\n",
        "print(train_dataset[0][\"prompt\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Define Custom Reward Function\n",
        "\n",
        "Asymmetric reward structure for class-imbalanced binary classification:\n",
        "- **True Positive** (pred=1, actual=1): +1\n",
        "- **True Negative** (pred=0, actual=0): +3\n",
        "- **False Positive** (pred=1, actual=0): -1\n",
        "- **False Negative** (pred=0, actual=1): -3\n",
        "\n",
        "This reward structure penalizes missing anomalies more heavily than false alarms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hdfs_classification_reward(completions, label, **kwargs):\n",
        "    \"\"\"\n",
        "    Asymmetric reward for class-imbalanced binary classification.\n",
        "    \n",
        "    Args:\n",
        "        completions: List of model completions (strings or conversation format)\n",
        "        label: List of ground truth labels (0 or 1)\n",
        "    \n",
        "    Returns:\n",
        "        List of reward values\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for completion, actual in zip(completions, label):\n",
        "        # Handle conversational format\n",
        "        if isinstance(completion, list):\n",
        "            text = completion[0][\"content\"] if completion else \"\"\n",
        "        else:\n",
        "            text = completion\n",
        "        \n",
        "        pred = parse_prediction(text)\n",
        "        actual_bool = bool(actual)\n",
        "        \n",
        "        if pred == actual_bool:\n",
        "            # Correct prediction\n",
        "            reward = 3.0 if not actual_bool else 1.0  # TN=+3, TP=+1\n",
        "        else:\n",
        "            # Incorrect prediction\n",
        "            reward = -3.0 if actual_bool else -1.0  # FN=-3, FP=-1\n",
        "        \n",
        "        rewards.append(reward)\n",
        "    \n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model & Tokenizer Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"LiquidAI/LFM2.5-1.2B-Thinking\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Set padding token (required for batched training)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Required for GRPO\n",
        "\n",
        "print(f\"Tokenizer loaded: {MODEL_NAME}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA config for efficient fine-tuning\n",
        "peft_config = LoraConfig(\n",
        "    r=2,\n",
        "    lora_alpha=4,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "print(\"LoRA config:\")\n",
        "print(f\"  r={peft_config.r}, alpha={peft_config.lora_alpha}\")\n",
        "print(f\"  Target modules: {peft_config.target_modules}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Single example inference with full trace\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# from peft import PeftModel\n",
        "# import torch\n",
        "# from functions import parse_prediction\n",
        "\n",
        "# # Get example\n",
        "# example = val_sample[0]\n",
        "# print(f\"Label: {example['label']} ({'Anomalous' if example['label'] == 1 else 'Normal'})\")\n",
        "\n",
        "# # Load model\n",
        "# MODEL_NAME = \"LiquidAI/LFM2.5-1.2B-Thinking\"\n",
        "# OUTPUT_DIR = \"./grpo-hdfs-classifier-final\"\n",
        "\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     MODEL_NAME, dtype=torch.float16, trust_remote_code=True, device_map=\"auto\"\n",
        "# )\n",
        "# try:\n",
        "#     model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "# except:\n",
        "#     model = base_model\n",
        "# model.eval()\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# # Format and tokenize\n",
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": SYSTEM_PROMPT + \"\\n\\nLimit your thinking to under 512 characters.\"},\n",
        "#     {\"role\": \"user\", \"content\": example['text']}\n",
        "# ]\n",
        "# input_ids = tokenizer.apply_chat_template(\n",
        "#     messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "# )[\"input_ids\"].to(model.device)\n",
        "\n",
        "# # Generate token-by-token (full trace)\n",
        "# print(\"\\nGeneration trace:\")\n",
        "# generated_text = \"\"\n",
        "# current_ids = input_ids.clone()\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for _ in range(512):\n",
        "#         outputs = model(current_ids)\n",
        "#         next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)\n",
        "#         next_token = tokenizer.decode(next_token_id[0], skip_special_tokens=False)\n",
        "#         generated_text += next_token\n",
        "#         print(next_token, end=\"\", flush=True)\n",
        "#         current_ids = torch.cat([current_ids, next_token_id], dim=1)\n",
        "#         if next_token_id.item() == tokenizer.eos_token_id:\n",
        "#             break\n",
        "\n",
        "# # Parse and compare\n",
        "# pred = parse_prediction(generated_text)\n",
        "# print(f\"\\n\\nPrediction: {pred} | Ground Truth: {example['label']} | Match: {'✓' if (1 if pred else 0) == example['label'] else '✗'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# len(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Use original val_sample (with text field)\n",
        "# val_examples = list(val_sample)\n",
        "# val_labels = [ex[\"label\"] for ex in val_examples]\n",
        "\n",
        "# # Compare with base model (before fine-tuning)\n",
        "# print(\"\\nLoading base model for comparison...\")\n",
        "\n",
        "# base_model_eval = AutoModelForCausalLM.from_pretrained(\n",
        "#     MODEL_NAME,\n",
        "#     dtype=torch.float16,\n",
        "#     trust_remote_code=True,\n",
        "#     device_map=\"auto\"\n",
        "# )\n",
        "# base_model_eval.eval()\n",
        "\n",
        "# # Run inference with base model\n",
        "# print(\"Running evaluation with base model...\")\n",
        "# base_predictions = run_local_inference(base_model_eval, tokenizer, SYSTEM_PROMPT, val_examples)\n",
        "\n",
        "# # Calculate metrics\n",
        "# base_metrics = calculate_metrics(base_predictions, val_labels)\n",
        "\n",
        "# print(\"\\n\" + \"=\"*50)\n",
        "# print(\"Base Model Evaluation Results\")\n",
        "# print(\"=\"*50)\n",
        "# for metric, value in base_metrics.items():\n",
        "#     print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = GRPOConfig(\n",
        "    output_dir=\"./grpo-hdfs-classifier\",\n",
        "    \n",
        "    # Training params\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-6,\n",
        "    \n",
        "    # GRPO specific - MEMORY OPTIMIZED\n",
        "    max_completion_length=512,\n",
        "    num_generations=2,  # Reduced from 4 to 2 (minimum for GRPO, halves memory)\n",
        "    \n",
        "    # Logging & saving\n",
        "    logging_steps=5,\n",
        "    # save_steps=50,\n",
        "    \n",
        "    # Hardware optimization\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # Disable external logging\n",
        "    report_to=\"none\",\n",
        "\n",
        "    dataloader_pin_memory=False\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Generations per prompt: {training_args.num_generations}\")\n",
        "print(f\"  Max completion length: {training_args.max_completion_length}\")\n",
        "print(f\"  Max total length: {getattr(training_args, 'max_length', 'Not set')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Initialize & Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=MODEL_NAME,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    reward_funcs=hdfs_classification_reward,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Run training\n",
        "print(\"Starting GRPO training...\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "OUTPUT_DIR = \"./grpo-hdfs-classifier-final\"\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation\n",
        "\n",
        "Compare base model vs fine-tuned model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load fine-tuned model for evaluation\n",
        "print(\"Loading fine-tuned model...\")\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Load LoRA adapter\n",
        "finetuned_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "finetuned_model.eval()\n",
        "\n",
        "print(\"Fine-tuned model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on validation set\n",
        "print(\"Running evaluation on validation set...\")\n",
        "\n",
        "# Use original val_sample (with text field)\n",
        "val_examples = list(val_sample)\n",
        "val_labels = [ex[\"label\"] for ex in val_examples]\n",
        "\n",
        "# Run inference\n",
        "predictions = run_local_inference(finetuned_model, tokenizer, SYSTEM_PROMPT, val_examples)\n",
        "\n",
        "# Calculate metrics\n",
        "metrics = calculate_metrics(predictions, val_labels)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Fine-tuned Model Evaluation Results\")\n",
        "print(\"=\"*50)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
