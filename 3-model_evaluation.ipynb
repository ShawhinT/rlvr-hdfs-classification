{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506a3951-5091-4354-9569-0697bb431de4",
   "metadata": {},
   "source": [
    "# Compare Base vs Fine-Tuned Model\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5d633",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f5e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from functions import run_inference, calculate_metrics, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61048f29",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921ab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from HF hub\n",
    "ds = load_dataset(\"shawhin/HDFS_v1_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd633697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 5 anomalous, 45 normal\n"
     ]
    }
   ],
   "source": [
    "num_test = 50\n",
    "\n",
    "# Split testing data by class\n",
    "test_anomalous = ds[\"test\"].filter(lambda x: x[\"label\"] == 1).shuffle(seed=42)\n",
    "test_normal = ds[\"test\"].filter(lambda x: x[\"label\"] == 0).shuffle(seed=42)\n",
    "\n",
    "# Balanced 90-10 split for testing\n",
    "test_sample = concatenate_datasets([\n",
    "    test_anomalous.select(range(int(num_test * 0.1))),\n",
    "    test_normal.select(range(int(num_test * 0.9)))\n",
    "]).shuffle(seed=42)\n",
    "\n",
    "# compute test set distribution\n",
    "test_positive = sum(test_sample[\"label\"])\n",
    "\n",
    "print(f\"Test: {test_positive} anomalous, {len(test_sample) - test_positive} normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59de0a4",
   "metadata": {},
   "source": [
    "### response format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab3eaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define structured output schema\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"anomaly_flag\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"Anomalous\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"description\": \"True if the log block is anomalous, false otherwise.\"\n",
    "                },\n",
    "                \"reasoning\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Explanation of the decision regarding whether the block is anomalous.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"Anomalous\", \"reasoning\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42473f",
   "metadata": {},
   "source": [
    "### evaluate base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107a6af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running o4-mini-2025-04-16: 100%|███████████████| 50/50 [06:28<00:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model (o4-mini-2025-04-16) metrics:\n",
      "  accuracy: 0.7800\n",
      "  precision: 0.2000\n",
      "  recall: 0.4000\n",
      "  f1: 0.2667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = \"o4-mini-2025-04-16\"\n",
    "\n",
    "# run base model on validation set\n",
    "test_labels = [bool(ex[\"label\"]) for ex in test_sample]\n",
    "\n",
    "base_results = run_inference(base_model, test_sample, response_format)\n",
    "base_predictions = [r[\"Anomalous\"] for r in base_results]  # Extract booleans\n",
    "base_metrics = calculate_metrics(base_predictions, test_labels)\n",
    "\n",
    "print(f\"Base model ({base_model}) metrics:\")\n",
    "for metric, value in base_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c905a7",
   "metadata": {},
   "source": [
    "### evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28eeacb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ft:o4-mini-2025-04-16:shawhin-talebi-ventures-llc:hdfs-classification:D2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model (ft:o4-mini-2025-04-16:shawhin-talebi-ventures-llc:hdfs-classification:D2XE1CWu) metrics:\n",
      "  accuracy: 0.9200\n",
      "  precision: 0.5714\n",
      "  recall: 0.8000\n",
      "  f1: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ft_model = \"ft:o4-mini-2025-04-16:shawhin-talebi-ventures-llc:hdfs-classification:D2XE1CWu\"\n",
    "\n",
    "# run fine-tuned model on validation set\n",
    "ft_results = run_inference(ft_model, test_sample, response_format)\n",
    "ft_predictions = [r[\"Anomalous\"] for r in ft_results]  # Extract booleans\n",
    "ft_metrics = calculate_metrics(ft_predictions, test_labels)\n",
    "\n",
    "print(f\"Fine-tuned model ({ft_model}) metrics:\")\n",
    "for metric, value in ft_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2142b4a",
   "metadata": {},
   "source": [
    "### compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85f6641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n",
      "                  accuracy  precision  recall      f1\n",
      "Base Model            0.78     0.2000     0.4  0.2667\n",
      "Fine-tuned Model      0.92     0.5714     0.8  0.6667\n",
      "Improvement           0.14     0.3714     0.4  0.4000\n",
      "\n",
      "Confusion Matrices:\n",
      "\n",
      "Base Model:\n",
      "[[37  8]\n",
      " [ 3  2]]\n",
      "\n",
      "Fine-tuned Model:\n",
      "[[42  3]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "source": [
    "# compare base vs fine-tuned model\n",
    "comparison = pd.DataFrame({\n",
    "    \"Base Model\": base_metrics,\n",
    "    \"Fine-tuned Model\": ft_metrics\n",
    "}).T\n",
    "\n",
    "# add improvement row\n",
    "improvement = {k: ft_metrics[k] - base_metrics[k] for k in base_metrics}\n",
    "comparison.loc[\"Improvement\"] = improvement\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison.round(4).to_string())\n",
    "\n",
    "print(\"\\nConfusion Matrices:\")\n",
    "print(f\"\\nBase Model:\\n{confusion_matrix(test_labels, base_predictions)}\")\n",
    "print(f\"\\nFine-tuned Model:\\n{confusion_matrix(test_labels, ft_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cq7ft3lcyn",
   "metadata": {},
   "source": [
    "### save evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hkvw7zgc685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 evaluation results to data/eval_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create evaluation results DataFrame\n",
    "results_data = []\n",
    "for i, ex in enumerate(test_sample):\n",
    "    results_data.append({\n",
    "        \"block_id\": ex[\"block_id\"],\n",
    "        \"block_content\": ex[\"text\"],\n",
    "        \"baseline_reasoning\": base_results[i][\"reasoning\"],\n",
    "        \"baseline_prediction\": base_results[i][\"Anomalous\"],\n",
    "        \"ft_reasoning\": ft_results[i][\"reasoning\"],\n",
    "        \"ft_prediction\": ft_results[i][\"Anomalous\"],\n",
    "        \"ground_truth\": bool(ex[\"label\"])\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "results_df.to_csv(\"data/eval_results.csv\", index=False)\n",
    "print(f\"Saved {len(results_df)} evaluation results to data/eval_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv)",
   "language": "python",
   "name": "my-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
